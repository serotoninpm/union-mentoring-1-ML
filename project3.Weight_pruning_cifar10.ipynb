{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uXuITn8289eq"
   },
   "outputs": [],
   "source": [
    "#결과 inline에 출력\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FEH4NGb6-KeH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WbDcVbDtYQxy"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwxpmJonGrsb",
    "outputId": "0fab87ba-52f8-4d2a-88bf-81fec7f02dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers= 14 ,pin_memory= True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=14, pin_memory= True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "rgQfhxpXJZ5f",
    "outputId": "859af676-6ece-430a-a4ff-0b0346bd5bc7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 이미지를 보여주기 위한 함수\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 학습용 이미지를 무작위로 가져오기\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# 이미지 보여주기\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# 정답(label) 출력\n",
    "# print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NB7aQV8uJZv6",
    "outputId": "a827f329-4e7f-4478-b1c1-839409b5d2a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Net(\n",
       "    (layer1): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (3): ReLU()\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc1): Linear(in_features=1600, out_features=120, bias=True)\n",
       "    (bn1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "          nn.Conv2d(3, 32, 3),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(32, 32, 3),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2, 2),\n",
    "          nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "          nn.Conv2d(32, 64, 3),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(64, 64, 3),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2, 2),\n",
    "          nn.Dropout(0.2)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(1600, 120)\n",
    "        self.bn1 = nn.BatchNorm1d(120)\n",
    "        self.bn2 = nn.BatchNorm1d(84)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = torch.flatten(x, 1) # 배치를 제외한 모든 차원을 평탄화(flatten)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "module.layer1.0.weight \t torch.Size([32, 3, 3, 3])\n",
      "module.layer1.0.bias \t torch.Size([32])\n",
      "module.layer1.2.weight \t torch.Size([32, 32, 3, 3])\n",
      "module.layer1.2.bias \t torch.Size([32])\n",
      "module.layer2.0.weight \t torch.Size([64, 32, 3, 3])\n",
      "module.layer2.0.bias \t torch.Size([64])\n",
      "module.layer2.2.weight \t torch.Size([64, 64, 3, 3])\n",
      "module.layer2.2.bias \t torch.Size([64])\n",
      "module.fc1.weight \t torch.Size([120, 1600])\n",
      "module.fc1.bias \t torch.Size([120])\n",
      "module.bn1.weight \t torch.Size([120])\n",
      "module.bn1.bias \t torch.Size([120])\n",
      "module.bn1.running_mean \t torch.Size([120])\n",
      "module.bn1.running_var \t torch.Size([120])\n",
      "module.bn1.num_batches_tracked \t torch.Size([])\n",
      "module.bn2.weight \t torch.Size([84])\n",
      "module.bn2.bias \t torch.Size([84])\n",
      "module.bn2.running_mean \t torch.Size([84])\n",
      "module.bn2.running_var \t torch.Size([84])\n",
      "module.bn2.num_batches_tracked \t torch.Size([])\n",
      "module.fc2.weight \t torch.Size([84, 120])\n",
      "module.fc2.bias \t torch.Size([84])\n",
      "module.fc3.weight \t torch.Size([10, 84])\n",
      "module.fc3.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 모델의 state_dict 출력\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NmP7RK2FWytM"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPYggzAfW0Be",
    "outputId": "14b7fb32-dfdf-4530-c83d-f03b72f2b92a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/union2/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448278899/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 2.134\n",
      "[1,    20] loss: 1.811\n",
      "[1,    30] loss: 1.650\n",
      "[1,    40] loss: 1.538\n",
      "[2,    10] loss: 1.361\n",
      "[2,    20] loss: 1.286\n",
      "[2,    30] loss: 1.243\n",
      "[2,    40] loss: 1.177\n",
      "[3,    10] loss: 1.078\n",
      "[3,    20] loss: 1.058\n",
      "[3,    30] loss: 1.035\n",
      "[3,    40] loss: 1.007\n",
      "[4,    10] loss: 0.919\n",
      "[4,    20] loss: 0.903\n",
      "[4,    30] loss: 0.912\n",
      "[4,    40] loss: 0.886\n",
      "[5,    10] loss: 0.835\n",
      "[5,    20] loss: 0.809\n",
      "[5,    30] loss: 0.790\n",
      "[5,    40] loss: 0.807\n",
      "[6,    10] loss: 0.753\n",
      "[6,    20] loss: 0.730\n",
      "[6,    30] loss: 0.745\n",
      "[6,    40] loss: 0.713\n",
      "[7,    10] loss: 0.677\n",
      "[7,    20] loss: 0.677\n",
      "[7,    30] loss: 0.671\n",
      "[7,    40] loss: 0.679\n",
      "[8,    10] loss: 0.617\n",
      "[8,    20] loss: 0.641\n",
      "[8,    30] loss: 0.633\n",
      "[8,    40] loss: 0.640\n",
      "[9,    10] loss: 0.570\n",
      "[9,    20] loss: 0.591\n",
      "[9,    30] loss: 0.604\n",
      "[9,    40] loss: 0.588\n",
      "[10,    10] loss: 0.556\n",
      "[10,    20] loss: 0.535\n",
      "[10,    30] loss: 0.548\n",
      "[10,    40] loss: 0.584\n",
      "[11,    10] loss: 0.511\n",
      "[11,    20] loss: 0.508\n",
      "[11,    30] loss: 0.526\n",
      "[11,    40] loss: 0.522\n",
      "[12,    10] loss: 0.479\n",
      "[12,    20] loss: 0.478\n",
      "[12,    30] loss: 0.498\n",
      "[12,    40] loss: 0.499\n",
      "[13,    10] loss: 0.447\n",
      "[13,    20] loss: 0.465\n",
      "[13,    30] loss: 0.469\n",
      "[13,    40] loss: 0.481\n",
      "[14,    10] loss: 0.425\n",
      "[14,    20] loss: 0.432\n",
      "[14,    30] loss: 0.448\n",
      "[14,    40] loss: 0.437\n",
      "[15,    10] loss: 0.387\n",
      "[15,    20] loss: 0.406\n",
      "[15,    30] loss: 0.416\n",
      "[15,    40] loss: 0.426\n",
      "[16,    10] loss: 0.375\n",
      "[16,    20] loss: 0.378\n",
      "[16,    30] loss: 0.406\n",
      "[16,    40] loss: 0.424\n",
      "[17,    10] loss: 0.365\n",
      "[17,    20] loss: 0.370\n",
      "[17,    30] loss: 0.384\n",
      "[17,    40] loss: 0.379\n",
      "[18,    10] loss: 0.324\n",
      "[18,    20] loss: 0.342\n",
      "[18,    30] loss: 0.366\n",
      "[18,    40] loss: 0.381\n",
      "[19,    10] loss: 0.320\n",
      "[19,    20] loss: 0.330\n",
      "[19,    30] loss: 0.341\n",
      "[19,    40] loss: 0.359\n",
      "[20,    10] loss: 0.310\n",
      "[20,    20] loss: 0.313\n",
      "[20,    30] loss: 0.333\n",
      "[20,    40] loss: 0.338\n",
      "[21,    10] loss: 0.296\n",
      "[21,    20] loss: 0.312\n",
      "[21,    30] loss: 0.320\n",
      "[21,    40] loss: 0.335\n",
      "[22,    10] loss: 0.305\n",
      "[22,    20] loss: 0.300\n",
      "[22,    30] loss: 0.309\n",
      "[22,    40] loss: 0.318\n",
      "[23,    10] loss: 0.268\n",
      "[23,    20] loss: 0.281\n",
      "[23,    30] loss: 0.288\n",
      "[23,    40] loss: 0.284\n",
      "[24,    10] loss: 0.261\n",
      "[24,    20] loss: 0.269\n",
      "[24,    30] loss: 0.291\n",
      "[24,    40] loss: 0.284\n",
      "[25,    10] loss: 0.263\n",
      "[25,    20] loss: 0.260\n",
      "[25,    30] loss: 0.273\n",
      "[25,    40] loss: 0.282\n",
      "[26,    10] loss: 0.237\n",
      "[26,    20] loss: 0.249\n",
      "[26,    30] loss: 0.260\n",
      "[26,    40] loss: 0.257\n",
      "[27,    10] loss: 0.242\n",
      "[27,    20] loss: 0.247\n",
      "[27,    30] loss: 0.254\n",
      "[27,    40] loss: 0.264\n",
      "[28,    10] loss: 0.234\n",
      "[28,    20] loss: 0.237\n",
      "[28,    30] loss: 0.245\n",
      "[28,    40] loss: 0.252\n",
      "[29,    10] loss: 0.213\n",
      "[29,    20] loss: 0.219\n",
      "[29,    30] loss: 0.233\n",
      "[29,    40] loss: 0.247\n",
      "[30,    10] loss: 0.206\n",
      "[30,    20] loss: 0.224\n",
      "[30,    30] loss: 0.228\n",
      "[30,    40] loss: 0.239\n",
      "[31,    10] loss: 0.207\n",
      "[31,    20] loss: 0.208\n",
      "[31,    30] loss: 0.222\n",
      "[31,    40] loss: 0.233\n",
      "[32,    10] loss: 0.186\n",
      "[32,    20] loss: 0.205\n",
      "[32,    30] loss: 0.210\n",
      "[32,    40] loss: 0.219\n",
      "[33,    10] loss: 0.197\n",
      "[33,    20] loss: 0.198\n",
      "[33,    30] loss: 0.212\n",
      "[33,    40] loss: 0.221\n",
      "[34,    10] loss: 0.185\n",
      "[34,    20] loss: 0.191\n",
      "[34,    30] loss: 0.200\n",
      "[34,    40] loss: 0.218\n",
      "[35,    10] loss: 0.175\n",
      "[35,    20] loss: 0.198\n",
      "[35,    30] loss: 0.201\n",
      "[35,    40] loss: 0.205\n",
      "[36,    10] loss: 0.176\n",
      "[36,    20] loss: 0.183\n",
      "[36,    30] loss: 0.181\n",
      "[36,    40] loss: 0.198\n",
      "[37,    10] loss: 0.170\n",
      "[37,    20] loss: 0.185\n",
      "[37,    30] loss: 0.177\n",
      "[37,    40] loss: 0.190\n",
      "[38,    10] loss: 0.179\n",
      "[38,    20] loss: 0.174\n",
      "[38,    30] loss: 0.184\n",
      "[38,    40] loss: 0.201\n",
      "[39,    10] loss: 0.171\n",
      "[39,    20] loss: 0.176\n",
      "[39,    30] loss: 0.190\n",
      "[39,    40] loss: 0.190\n",
      "[40,    10] loss: 0.178\n",
      "[40,    20] loss: 0.163\n",
      "[40,    30] loss: 0.172\n",
      "[40,    40] loss: 0.167\n",
      "[41,    10] loss: 0.164\n",
      "[41,    20] loss: 0.166\n",
      "[41,    30] loss: 0.177\n",
      "[41,    40] loss: 0.177\n",
      "[42,    10] loss: 0.164\n",
      "[42,    20] loss: 0.165\n",
      "[42,    30] loss: 0.177\n",
      "[42,    40] loss: 0.174\n",
      "[43,    10] loss: 0.161\n",
      "[43,    20] loss: 0.163\n",
      "[43,    30] loss: 0.168\n",
      "[43,    40] loss: 0.165\n",
      "[44,    10] loss: 0.167\n",
      "[44,    20] loss: 0.155\n",
      "[44,    30] loss: 0.168\n",
      "[44,    40] loss: 0.168\n",
      "[45,    10] loss: 0.151\n",
      "[45,    20] loss: 0.165\n",
      "[45,    30] loss: 0.146\n",
      "[45,    40] loss: 0.166\n",
      "[46,    10] loss: 0.136\n",
      "[46,    20] loss: 0.150\n",
      "[46,    30] loss: 0.152\n",
      "[46,    40] loss: 0.157\n",
      "[47,    10] loss: 0.140\n",
      "[47,    20] loss: 0.139\n",
      "[47,    30] loss: 0.144\n",
      "[47,    40] loss: 0.164\n",
      "[48,    10] loss: 0.141\n",
      "[48,    20] loss: 0.145\n",
      "[48,    30] loss: 0.151\n",
      "[48,    40] loss: 0.160\n",
      "[49,    10] loss: 0.142\n",
      "[49,    20] loss: 0.139\n",
      "[49,    30] loss: 0.142\n",
      "[49,    40] loss: 0.155\n",
      "[50,    10] loss: 0.145\n",
      "[50,    20] loss: 0.138\n",
      "[50,    30] loss: 0.153\n",
      "[50,    40] loss: 0.147\n",
      "[51,    10] loss: 0.136\n",
      "[51,    20] loss: 0.140\n",
      "[51,    30] loss: 0.146\n",
      "[51,    40] loss: 0.147\n",
      "[52,    10] loss: 0.132\n",
      "[52,    20] loss: 0.137\n",
      "[52,    30] loss: 0.150\n",
      "[52,    40] loss: 0.148\n",
      "[53,    10] loss: 0.148\n",
      "[53,    20] loss: 0.143\n",
      "[53,    30] loss: 0.151\n",
      "[53,    40] loss: 0.134\n",
      "[54,    10] loss: 0.126\n",
      "[54,    20] loss: 0.133\n",
      "[54,    30] loss: 0.136\n",
      "[54,    40] loss: 0.146\n",
      "[55,    10] loss: 0.128\n",
      "[55,    20] loss: 0.130\n",
      "[55,    30] loss: 0.137\n",
      "[55,    40] loss: 0.143\n",
      "[56,    10] loss: 0.122\n",
      "[56,    20] loss: 0.130\n",
      "[56,    30] loss: 0.136\n",
      "[56,    40] loss: 0.147\n",
      "[57,    10] loss: 0.123\n",
      "[57,    20] loss: 0.126\n",
      "[57,    30] loss: 0.134\n",
      "[57,    40] loss: 0.128\n",
      "[58,    10] loss: 0.124\n",
      "[58,    20] loss: 0.123\n",
      "[58,    30] loss: 0.137\n",
      "[58,    40] loss: 0.131\n",
      "[59,    10] loss: 0.129\n",
      "[59,    20] loss: 0.129\n",
      "[59,    30] loss: 0.133\n",
      "[59,    40] loss: 0.145\n",
      "[60,    10] loss: 0.129\n",
      "[60,    20] loss: 0.122\n",
      "[60,    30] loss: 0.127\n",
      "[60,    40] loss: 0.125\n",
      "[61,    10] loss: 0.121\n",
      "[61,    20] loss: 0.116\n",
      "[61,    30] loss: 0.124\n",
      "[61,    40] loss: 0.125\n",
      "[62,    10] loss: 0.122\n",
      "[62,    20] loss: 0.122\n",
      "[62,    30] loss: 0.118\n",
      "[62,    40] loss: 0.137\n",
      "[63,    10] loss: 0.121\n",
      "[63,    20] loss: 0.122\n",
      "[63,    30] loss: 0.124\n",
      "[63,    40] loss: 0.128\n",
      "[64,    10] loss: 0.121\n",
      "[64,    20] loss: 0.117\n",
      "[64,    30] loss: 0.122\n",
      "[64,    40] loss: 0.119\n",
      "[65,    10] loss: 0.120\n",
      "[65,    20] loss: 0.117\n",
      "[65,    30] loss: 0.116\n",
      "[65,    40] loss: 0.128\n",
      "[66,    10] loss: 0.112\n",
      "[66,    20] loss: 0.116\n",
      "[66,    30] loss: 0.119\n",
      "[66,    40] loss: 0.125\n",
      "[67,    10] loss: 0.113\n",
      "[67,    20] loss: 0.113\n",
      "[67,    30] loss: 0.119\n",
      "[67,    40] loss: 0.116\n",
      "[68,    10] loss: 0.109\n",
      "[68,    20] loss: 0.114\n",
      "[68,    30] loss: 0.115\n",
      "[68,    40] loss: 0.126\n",
      "[69,    10] loss: 0.108\n",
      "[69,    20] loss: 0.118\n",
      "[69,    30] loss: 0.123\n",
      "[69,    40] loss: 0.114\n",
      "[70,    10] loss: 0.111\n",
      "[70,    20] loss: 0.112\n",
      "[70,    30] loss: 0.119\n",
      "[70,    40] loss: 0.117\n",
      "[71,    10] loss: 0.102\n",
      "[71,    20] loss: 0.106\n",
      "[71,    30] loss: 0.105\n",
      "[71,    40] loss: 0.115\n",
      "[72,    10] loss: 0.097\n",
      "[72,    20] loss: 0.101\n",
      "[72,    30] loss: 0.114\n",
      "[72,    40] loss: 0.115\n",
      "[73,    10] loss: 0.103\n",
      "[73,    20] loss: 0.107\n",
      "[73,    30] loss: 0.109\n",
      "[73,    40] loss: 0.100\n",
      "[74,    10] loss: 0.111\n",
      "[74,    20] loss: 0.112\n",
      "[74,    30] loss: 0.119\n",
      "[74,    40] loss: 0.118\n",
      "[75,    10] loss: 0.105\n",
      "[75,    20] loss: 0.107\n",
      "[75,    30] loss: 0.105\n",
      "[75,    40] loss: 0.118\n",
      "[76,    10] loss: 0.110\n",
      "[76,    20] loss: 0.109\n",
      "[76,    30] loss: 0.111\n",
      "[76,    40] loss: 0.120\n",
      "[77,    10] loss: 0.100\n",
      "[77,    20] loss: 0.104\n",
      "[77,    30] loss: 0.108\n",
      "[77,    40] loss: 0.115\n",
      "[78,    10] loss: 0.088\n",
      "[78,    20] loss: 0.097\n",
      "[78,    30] loss: 0.094\n",
      "[78,    40] loss: 0.103\n",
      "[79,    10] loss: 0.100\n",
      "[79,    20] loss: 0.104\n",
      "[79,    30] loss: 0.107\n",
      "[79,    40] loss: 0.105\n",
      "[80,    10] loss: 0.094\n",
      "[80,    20] loss: 0.101\n",
      "[80,    30] loss: 0.096\n",
      "[80,    40] loss: 0.102\n",
      "[81,    10] loss: 0.097\n",
      "[81,    20] loss: 0.102\n",
      "[81,    30] loss: 0.100\n",
      "[81,    40] loss: 0.106\n",
      "[82,    10] loss: 0.101\n",
      "[82,    20] loss: 0.100\n",
      "[82,    30] loss: 0.106\n",
      "[82,    40] loss: 0.097\n",
      "[83,    10] loss: 0.094\n",
      "[83,    20] loss: 0.096\n",
      "[83,    30] loss: 0.104\n",
      "[83,    40] loss: 0.105\n",
      "[84,    10] loss: 0.095\n",
      "[84,    20] loss: 0.112\n",
      "[84,    30] loss: 0.101\n",
      "[84,    40] loss: 0.109\n",
      "[85,    10] loss: 0.104\n",
      "[85,    20] loss: 0.102\n",
      "[85,    30] loss: 0.106\n",
      "[85,    40] loss: 0.112\n",
      "[86,    10] loss: 0.094\n",
      "[86,    20] loss: 0.101\n",
      "[86,    30] loss: 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86,    40] loss: 0.102\n",
      "[87,    10] loss: 0.101\n",
      "[87,    20] loss: 0.104\n",
      "[87,    30] loss: 0.097\n",
      "[87,    40] loss: 0.097\n",
      "[88,    10] loss: 0.096\n",
      "[88,    20] loss: 0.091\n",
      "[88,    30] loss: 0.098\n",
      "[88,    40] loss: 0.102\n",
      "[89,    10] loss: 0.095\n",
      "[89,    20] loss: 0.101\n",
      "[89,    30] loss: 0.100\n",
      "[89,    40] loss: 0.097\n",
      "[90,    10] loss: 0.089\n",
      "[90,    20] loss: 0.100\n",
      "[90,    30] loss: 0.104\n",
      "[90,    40] loss: 0.099\n",
      "[91,    10] loss: 0.093\n",
      "[91,    20] loss: 0.094\n",
      "[91,    30] loss: 0.093\n",
      "[91,    40] loss: 0.100\n",
      "[92,    10] loss: 0.090\n",
      "[92,    20] loss: 0.093\n",
      "[92,    30] loss: 0.097\n",
      "[92,    40] loss: 0.104\n",
      "[93,    10] loss: 0.091\n",
      "[93,    20] loss: 0.091\n",
      "[93,    30] loss: 0.083\n",
      "[93,    40] loss: 0.094\n",
      "[94,    10] loss: 0.095\n",
      "[94,    20] loss: 0.091\n",
      "[94,    30] loss: 0.092\n",
      "[94,    40] loss: 0.093\n",
      "[95,    10] loss: 0.092\n",
      "[95,    20] loss: 0.095\n",
      "[95,    30] loss: 0.097\n",
      "[95,    40] loss: 0.088\n",
      "[96,    10] loss: 0.091\n",
      "[96,    20] loss: 0.079\n",
      "[96,    30] loss: 0.091\n",
      "[96,    40] loss: 0.100\n",
      "[97,    10] loss: 0.086\n",
      "[97,    20] loss: 0.087\n",
      "[97,    30] loss: 0.091\n",
      "[97,    40] loss: 0.099\n",
      "[98,    10] loss: 0.092\n",
      "[98,    20] loss: 0.091\n",
      "[98,    30] loss: 0.098\n",
      "[98,    40] loss: 0.093\n",
      "[99,    10] loss: 0.085\n",
      "[99,    20] loss: 0.092\n",
      "[99,    30] loss: 0.088\n",
      "[99,    40] loss: 0.097\n",
      "[100,    10] loss: 0.090\n",
      "[100,    20] loss: 0.098\n",
      "[100,    30] loss: 0.098\n",
      "[100,    40] loss: 0.099\n",
      "[101,    10] loss: 0.078\n",
      "[101,    20] loss: 0.087\n",
      "[101,    30] loss: 0.078\n",
      "[101,    40] loss: 0.091\n",
      "[102,    10] loss: 0.082\n",
      "[102,    20] loss: 0.091\n",
      "[102,    30] loss: 0.094\n",
      "[102,    40] loss: 0.101\n",
      "[103,    10] loss: 0.087\n",
      "[103,    20] loss: 0.093\n",
      "[103,    30] loss: 0.101\n",
      "[103,    40] loss: 0.099\n",
      "[104,    10] loss: 0.085\n",
      "[104,    20] loss: 0.090\n",
      "[104,    30] loss: 0.100\n",
      "[104,    40] loss: 0.090\n",
      "[105,    10] loss: 0.089\n",
      "[105,    20] loss: 0.086\n",
      "[105,    30] loss: 0.087\n",
      "[105,    40] loss: 0.092\n",
      "[106,    10] loss: 0.086\n",
      "[106,    20] loss: 0.083\n",
      "[106,    30] loss: 0.090\n",
      "[106,    40] loss: 0.091\n",
      "[107,    10] loss: 0.082\n",
      "[107,    20] loss: 0.082\n",
      "[107,    30] loss: 0.090\n",
      "[107,    40] loss: 0.086\n",
      "[108,    10] loss: 0.081\n",
      "[108,    20] loss: 0.081\n",
      "[108,    30] loss: 0.090\n",
      "[108,    40] loss: 0.094\n",
      "[109,    10] loss: 0.083\n",
      "[109,    20] loss: 0.079\n",
      "[109,    30] loss: 0.079\n",
      "[109,    40] loss: 0.094\n",
      "[110,    10] loss: 0.084\n",
      "[110,    20] loss: 0.093\n",
      "[110,    30] loss: 0.098\n",
      "[110,    40] loss: 0.094\n",
      "[111,    10] loss: 0.081\n",
      "[111,    20] loss: 0.080\n",
      "[111,    30] loss: 0.085\n",
      "[111,    40] loss: 0.095\n",
      "[112,    10] loss: 0.076\n",
      "[112,    20] loss: 0.084\n",
      "[112,    30] loss: 0.090\n",
      "[112,    40] loss: 0.088\n",
      "[113,    10] loss: 0.085\n",
      "[113,    20] loss: 0.084\n",
      "[113,    30] loss: 0.098\n",
      "[113,    40] loss: 0.086\n",
      "[114,    10] loss: 0.081\n",
      "[114,    20] loss: 0.084\n",
      "[114,    30] loss: 0.082\n",
      "[114,    40] loss: 0.091\n",
      "[115,    10] loss: 0.075\n",
      "[115,    20] loss: 0.077\n",
      "[115,    30] loss: 0.090\n",
      "[115,    40] loss: 0.076\n",
      "[116,    10] loss: 0.082\n",
      "[116,    20] loss: 0.078\n",
      "[116,    30] loss: 0.087\n",
      "[116,    40] loss: 0.091\n",
      "[117,    10] loss: 0.076\n",
      "[117,    20] loss: 0.080\n",
      "[117,    30] loss: 0.084\n",
      "[117,    40] loss: 0.086\n",
      "[118,    10] loss: 0.084\n",
      "[118,    20] loss: 0.082\n",
      "[118,    30] loss: 0.085\n",
      "[118,    40] loss: 0.090\n",
      "[119,    10] loss: 0.082\n",
      "[119,    20] loss: 0.077\n",
      "[119,    30] loss: 0.077\n",
      "[119,    40] loss: 0.091\n",
      "[120,    10] loss: 0.081\n",
      "[120,    20] loss: 0.084\n",
      "[120,    30] loss: 0.083\n",
      "[120,    40] loss: 0.086\n",
      "[121,    10] loss: 0.080\n",
      "[121,    20] loss: 0.078\n",
      "[121,    30] loss: 0.082\n",
      "[121,    40] loss: 0.080\n",
      "[122,    10] loss: 0.078\n",
      "[122,    20] loss: 0.071\n",
      "[122,    30] loss: 0.079\n",
      "[122,    40] loss: 0.078\n",
      "[123,    10] loss: 0.071\n",
      "[123,    20] loss: 0.078\n",
      "[123,    30] loss: 0.078\n",
      "[123,    40] loss: 0.084\n",
      "[124,    10] loss: 0.077\n",
      "[124,    20] loss: 0.080\n",
      "[124,    30] loss: 0.074\n",
      "[124,    40] loss: 0.093\n",
      "[125,    10] loss: 0.086\n",
      "[125,    20] loss: 0.082\n",
      "[125,    30] loss: 0.076\n",
      "[125,    40] loss: 0.077\n",
      "[126,    10] loss: 0.069\n",
      "[126,    20] loss: 0.076\n",
      "[126,    30] loss: 0.076\n",
      "[126,    40] loss: 0.079\n",
      "[127,    10] loss: 0.084\n",
      "[127,    20] loss: 0.078\n",
      "[127,    30] loss: 0.093\n",
      "[127,    40] loss: 0.079\n",
      "[128,    10] loss: 0.079\n",
      "[128,    20] loss: 0.081\n",
      "[128,    30] loss: 0.075\n",
      "[128,    40] loss: 0.076\n",
      "[129,    10] loss: 0.071\n",
      "[129,    20] loss: 0.082\n",
      "[129,    30] loss: 0.082\n",
      "[129,    40] loss: 0.084\n",
      "[130,    10] loss: 0.078\n",
      "[130,    20] loss: 0.081\n",
      "[130,    30] loss: 0.080\n",
      "[130,    40] loss: 0.083\n",
      "[131,    10] loss: 0.069\n",
      "[131,    20] loss: 0.075\n",
      "[131,    30] loss: 0.073\n",
      "[131,    40] loss: 0.081\n",
      "[132,    10] loss: 0.074\n",
      "[132,    20] loss: 0.074\n",
      "[132,    30] loss: 0.076\n",
      "[132,    40] loss: 0.090\n",
      "[133,    10] loss: 0.079\n",
      "[133,    20] loss: 0.074\n",
      "[133,    30] loss: 0.077\n",
      "[133,    40] loss: 0.082\n",
      "[134,    10] loss: 0.071\n",
      "[134,    20] loss: 0.064\n",
      "[134,    30] loss: 0.063\n",
      "[134,    40] loss: 0.077\n",
      "[135,    10] loss: 0.074\n",
      "[135,    20] loss: 0.076\n",
      "[135,    30] loss: 0.079\n",
      "[135,    40] loss: 0.078\n",
      "[136,    10] loss: 0.072\n",
      "[136,    20] loss: 0.079\n",
      "[136,    30] loss: 0.076\n",
      "[136,    40] loss: 0.079\n",
      "[137,    10] loss: 0.073\n",
      "[137,    20] loss: 0.072\n",
      "[137,    30] loss: 0.071\n",
      "[137,    40] loss: 0.072\n",
      "[138,    10] loss: 0.077\n",
      "[138,    20] loss: 0.075\n",
      "[138,    30] loss: 0.078\n",
      "[138,    40] loss: 0.080\n",
      "[139,    10] loss: 0.072\n",
      "[139,    20] loss: 0.081\n",
      "[139,    30] loss: 0.078\n",
      "[139,    40] loss: 0.078\n",
      "[140,    10] loss: 0.075\n",
      "[140,    20] loss: 0.077\n",
      "[140,    30] loss: 0.069\n",
      "[140,    40] loss: 0.078\n",
      "[141,    10] loss: 0.069\n",
      "[141,    20] loss: 0.070\n",
      "[141,    30] loss: 0.075\n",
      "[141,    40] loss: 0.076\n",
      "[142,    10] loss: 0.068\n",
      "[142,    20] loss: 0.067\n",
      "[142,    30] loss: 0.073\n",
      "[142,    40] loss: 0.078\n",
      "[143,    10] loss: 0.069\n",
      "[143,    20] loss: 0.069\n",
      "[143,    30] loss: 0.077\n",
      "[143,    40] loss: 0.077\n",
      "[144,    10] loss: 0.079\n",
      "[144,    20] loss: 0.072\n",
      "[144,    30] loss: 0.081\n",
      "[144,    40] loss: 0.076\n",
      "[145,    10] loss: 0.066\n",
      "[145,    20] loss: 0.072\n",
      "[145,    30] loss: 0.076\n",
      "[145,    40] loss: 0.077\n",
      "[146,    10] loss: 0.063\n",
      "[146,    20] loss: 0.071\n",
      "[146,    30] loss: 0.075\n",
      "[146,    40] loss: 0.071\n",
      "[147,    10] loss: 0.066\n",
      "[147,    20] loss: 0.067\n",
      "[147,    30] loss: 0.078\n",
      "[147,    40] loss: 0.073\n",
      "[148,    10] loss: 0.073\n",
      "[148,    20] loss: 0.072\n",
      "[148,    30] loss: 0.068\n",
      "[148,    40] loss: 0.075\n",
      "[149,    10] loss: 0.061\n",
      "[149,    20] loss: 0.067\n",
      "[149,    30] loss: 0.071\n",
      "[149,    40] loss: 0.072\n",
      "[150,    10] loss: 0.067\n",
      "[150,    20] loss: 0.069\n",
      "[150,    30] loss: 0.069\n",
      "[150,    40] loss: 0.069\n",
      "[151,    10] loss: 0.081\n",
      "[151,    20] loss: 0.071\n",
      "[151,    30] loss: 0.068\n",
      "[151,    40] loss: 0.073\n",
      "[152,    10] loss: 0.071\n",
      "[152,    20] loss: 0.078\n",
      "[152,    30] loss: 0.076\n",
      "[152,    40] loss: 0.078\n",
      "[153,    10] loss: 0.070\n",
      "[153,    20] loss: 0.081\n",
      "[153,    30] loss: 0.076\n",
      "[153,    40] loss: 0.073\n",
      "[154,    10] loss: 0.069\n",
      "[154,    20] loss: 0.077\n",
      "[154,    30] loss: 0.082\n",
      "[154,    40] loss: 0.081\n",
      "[155,    10] loss: 0.070\n",
      "[155,    20] loss: 0.078\n",
      "[155,    30] loss: 0.075\n",
      "[155,    40] loss: 0.076\n",
      "[156,    10] loss: 0.064\n",
      "[156,    20] loss: 0.063\n",
      "[156,    30] loss: 0.075\n",
      "[156,    40] loss: 0.073\n",
      "[157,    10] loss: 0.067\n",
      "[157,    20] loss: 0.065\n",
      "[157,    30] loss: 0.069\n",
      "[157,    40] loss: 0.068\n",
      "[158,    10] loss: 0.064\n",
      "[158,    20] loss: 0.062\n",
      "[158,    30] loss: 0.067\n",
      "[158,    40] loss: 0.071\n",
      "[159,    10] loss: 0.067\n",
      "[159,    20] loss: 0.069\n",
      "[159,    30] loss: 0.073\n",
      "[159,    40] loss: 0.074\n",
      "[160,    10] loss: 0.067\n",
      "[160,    20] loss: 0.069\n",
      "[160,    30] loss: 0.072\n",
      "[160,    40] loss: 0.074\n",
      "[161,    10] loss: 0.066\n",
      "[161,    20] loss: 0.073\n",
      "[161,    30] loss: 0.070\n",
      "[161,    40] loss: 0.075\n",
      "[162,    10] loss: 0.065\n",
      "[162,    20] loss: 0.072\n",
      "[162,    30] loss: 0.069\n",
      "[162,    40] loss: 0.074\n",
      "[163,    10] loss: 0.066\n",
      "[163,    20] loss: 0.068\n",
      "[163,    30] loss: 0.080\n",
      "[163,    40] loss: 0.078\n",
      "[164,    10] loss: 0.071\n",
      "[164,    20] loss: 0.067\n",
      "[164,    30] loss: 0.070\n",
      "[164,    40] loss: 0.069\n",
      "[165,    10] loss: 0.070\n",
      "[165,    20] loss: 0.074\n",
      "[165,    30] loss: 0.074\n",
      "[165,    40] loss: 0.068\n",
      "[166,    10] loss: 0.061\n",
      "[166,    20] loss: 0.070\n",
      "[166,    30] loss: 0.069\n",
      "[166,    40] loss: 0.067\n",
      "[167,    10] loss: 0.063\n",
      "[167,    20] loss: 0.067\n",
      "[167,    30] loss: 0.069\n",
      "[167,    40] loss: 0.068\n",
      "[168,    10] loss: 0.058\n",
      "[168,    20] loss: 0.069\n",
      "[168,    30] loss: 0.069\n",
      "[168,    40] loss: 0.068\n",
      "[169,    10] loss: 0.074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169,    20] loss: 0.065\n",
      "[169,    30] loss: 0.057\n",
      "[169,    40] loss: 0.070\n",
      "[170,    10] loss: 0.064\n",
      "[170,    20] loss: 0.069\n",
      "[170,    30] loss: 0.066\n",
      "[170,    40] loss: 0.067\n",
      "[171,    10] loss: 0.067\n",
      "[171,    20] loss: 0.062\n",
      "[171,    30] loss: 0.063\n",
      "[171,    40] loss: 0.068\n",
      "[172,    10] loss: 0.065\n",
      "[172,    20] loss: 0.070\n",
      "[172,    30] loss: 0.066\n",
      "[172,    40] loss: 0.069\n",
      "[173,    10] loss: 0.062\n",
      "[173,    20] loss: 0.071\n",
      "[173,    30] loss: 0.065\n",
      "[173,    40] loss: 0.067\n",
      "[174,    10] loss: 0.067\n",
      "[174,    20] loss: 0.061\n",
      "[174,    30] loss: 0.065\n",
      "[174,    40] loss: 0.063\n",
      "[175,    10] loss: 0.068\n",
      "[175,    20] loss: 0.065\n",
      "[175,    30] loss: 0.069\n",
      "[175,    40] loss: 0.072\n",
      "[176,    10] loss: 0.064\n",
      "[176,    20] loss: 0.069\n",
      "[176,    30] loss: 0.067\n",
      "[176,    40] loss: 0.065\n",
      "[177,    10] loss: 0.055\n",
      "[177,    20] loss: 0.071\n",
      "[177,    30] loss: 0.068\n",
      "[177,    40] loss: 0.071\n",
      "[178,    10] loss: 0.065\n",
      "[178,    20] loss: 0.054\n",
      "[178,    30] loss: 0.065\n",
      "[178,    40] loss: 0.065\n",
      "[179,    10] loss: 0.065\n",
      "[179,    20] loss: 0.064\n",
      "[179,    30] loss: 0.062\n",
      "[179,    40] loss: 0.069\n",
      "[180,    10] loss: 0.056\n",
      "[180,    20] loss: 0.067\n",
      "[180,    30] loss: 0.065\n",
      "[180,    40] loss: 0.063\n",
      "[181,    10] loss: 0.066\n",
      "[181,    20] loss: 0.066\n",
      "[181,    30] loss: 0.066\n",
      "[181,    40] loss: 0.066\n",
      "[182,    10] loss: 0.063\n",
      "[182,    20] loss: 0.069\n",
      "[182,    30] loss: 0.063\n",
      "[182,    40] loss: 0.069\n",
      "[183,    10] loss: 0.073\n",
      "[183,    20] loss: 0.061\n",
      "[183,    30] loss: 0.062\n",
      "[183,    40] loss: 0.072\n",
      "[184,    10] loss: 0.064\n",
      "[184,    20] loss: 0.069\n",
      "[184,    30] loss: 0.070\n",
      "[184,    40] loss: 0.071\n",
      "[185,    10] loss: 0.061\n",
      "[185,    20] loss: 0.062\n",
      "[185,    30] loss: 0.065\n",
      "[185,    40] loss: 0.063\n",
      "[186,    10] loss: 0.070\n",
      "[186,    20] loss: 0.061\n",
      "[186,    30] loss: 0.067\n",
      "[186,    40] loss: 0.061\n",
      "[187,    10] loss: 0.059\n",
      "[187,    20] loss: 0.063\n",
      "[187,    30] loss: 0.062\n",
      "[187,    40] loss: 0.065\n",
      "[188,    10] loss: 0.055\n",
      "[188,    20] loss: 0.062\n",
      "[188,    30] loss: 0.068\n",
      "[188,    40] loss: 0.061\n",
      "[189,    10] loss: 0.056\n",
      "[189,    20] loss: 0.063\n",
      "[189,    30] loss: 0.059\n",
      "[189,    40] loss: 0.062\n",
      "[190,    10] loss: 0.057\n",
      "[190,    20] loss: 0.057\n",
      "[190,    30] loss: 0.062\n",
      "[190,    40] loss: 0.063\n",
      "[191,    10] loss: 0.058\n",
      "[191,    20] loss: 0.058\n",
      "[191,    30] loss: 0.060\n",
      "[191,    40] loss: 0.062\n",
      "[192,    10] loss: 0.062\n",
      "[192,    20] loss: 0.068\n",
      "[192,    30] loss: 0.058\n",
      "[192,    40] loss: 0.068\n",
      "[193,    10] loss: 0.058\n",
      "[193,    20] loss: 0.061\n",
      "[193,    30] loss: 0.059\n",
      "[193,    40] loss: 0.061\n",
      "[194,    10] loss: 0.062\n",
      "[194,    20] loss: 0.061\n",
      "[194,    30] loss: 0.063\n",
      "[194,    40] loss: 0.067\n",
      "[195,    10] loss: 0.069\n",
      "[195,    20] loss: 0.062\n",
      "[195,    30] loss: 0.063\n",
      "[195,    40] loss: 0.061\n",
      "[196,    10] loss: 0.064\n",
      "[196,    20] loss: 0.056\n",
      "[196,    30] loss: 0.059\n",
      "[196,    40] loss: 0.066\n",
      "[197,    10] loss: 0.059\n",
      "[197,    20] loss: 0.058\n",
      "[197,    30] loss: 0.060\n",
      "[197,    40] loss: 0.064\n",
      "[198,    10] loss: 0.059\n",
      "[198,    20] loss: 0.062\n",
      "[198,    30] loss: 0.056\n",
      "[198,    40] loss: 0.061\n",
      "[199,    10] loss: 0.054\n",
      "[199,    20] loss: 0.061\n",
      "[199,    30] loss: 0.057\n",
      "[199,    40] loss: 0.062\n",
      "[200,    10] loss: 0.062\n",
      "[200,    20] loss: 0.057\n",
      "[200,    30] loss: 0.066\n",
      "[200,    40] loss: 0.064\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "for epoch in range(200):   # 데이터셋을 수차례 반복합니다.\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # [inputs, labels]의 목록인 data로부터 입력을 받은 후;\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 변화도(Gradient) 매개변수를 0으로 만들고\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화를 한 후\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 통계를 출력합니다.\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SYFO6TofWz-0"
   },
   "outputs": [],
   "source": [
    "PATH = './cifar10_net_weight_pruning_test.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xi8HjximWz5-",
    "outputId": "688f3e8e-f55b-4d0d-b51c-d29a5e2c8c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroundTruth:    cat  ship  ship plane  frog  frog   car  frog   cat   car plane truck   dog horse truck  ship   dog horse  ship  frog horse plane  deer truck   dog  bird  deer plane truck  frog  frog   dog  deer   dog truck  bird  deer   car truck   dog  deer  frog   dog  frog plane truck   cat truck horse  frog truck  ship plane   cat  ship  ship horse horse  deer  frog horse   cat  frog   cat\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "# 이미지를 출력합니다.\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eAQ14i67Xzdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fKdJHWbX2DR",
    "outputId": "038c26f6-4ab3-4954-c104-504a9fdd2d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 79 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSiYXtBCYEJx",
    "outputId": "26710488-3eac-49fe-defb-67c03011007c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class plane is: 82.9 %\n",
      "Accuracy for class car   is: 89.9 %\n",
      "Accuracy for class bird  is: 65.9 %\n",
      "Accuracy for class cat   is: 57.9 %\n",
      "Accuracy for class deer  is: 76.5 %\n",
      "Accuracy for class dog   is: 69.9 %\n",
      "Accuracy for class frog  is: 82.9 %\n",
      "Accuracy for class horse is: 86.5 %\n",
      "Accuracy for class ship  is: 88.3 %\n",
      "Accuracy for class truck is: 90.3 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_node(torch_layer, percent):\n",
    "    #정렬된 weight(절댓값 기준)\n",
    "    abs_sort_result = torch.abs(torch_layer).view(-1).sort()\n",
    "    cut_off_index = int(len(abs_sort_result.values) * percent)\n",
    "    cut_off_value = abs_sort_result.values[cut_off_index-1]\n",
    "    return torch.where(torch.abs(torch_layer) > cut_off_value, torch_layer, torch.zeros(torch_layer.size(),dtype=torch.float32).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80퍼센트 pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "eAQ14i67Xzdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.8\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        cut = cut_node(param,percent)\n",
    "        net.state_dict()[name].data.copy_(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fKdJHWbX2DR",
    "outputId": "038c26f6-4ab3-4954-c104-504a9fdd2d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 9 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSiYXtBCYEJx",
    "outputId": "26710488-3eac-49fe-defb-67c03011007c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class plane is: 0.0 %\n",
      "Accuracy for class car   is: 0.0 %\n",
      "Accuracy for class bird  is: 0.0 %\n",
      "Accuracy for class cat   is: 0.0 %\n",
      "Accuracy for class deer  is: 0.0 %\n",
      "Accuracy for class dog   is: 95.0 %\n",
      "Accuracy for class frog  is: 2.9 %\n",
      "Accuracy for class horse is: 0.1 %\n",
      "Accuracy for class ship  is: 0.0 %\n",
      "Accuracy for class truck is: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50퍼센트 pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "eAQ14i67Xzdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.5\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        cut = cut_node(param,percent)\n",
    "        net.state_dict()[name].data.copy_(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fKdJHWbX2DR",
    "outputId": "038c26f6-4ab3-4954-c104-504a9fdd2d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSiYXtBCYEJx",
    "outputId": "26710488-3eac-49fe-defb-67c03011007c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class plane is: 46.9 %\n",
      "Accuracy for class car   is: 67.5 %\n",
      "Accuracy for class bird  is: 9.5 %\n",
      "Accuracy for class cat   is: 6.3 %\n",
      "Accuracy for class deer  is: 4.8 %\n",
      "Accuracy for class dog   is: 77.5 %\n",
      "Accuracy for class frog  is: 73.5 %\n",
      "Accuracy for class horse is: 25.0 %\n",
      "Accuracy for class ship  is: 24.9 %\n",
      "Accuracy for class truck is: 61.1 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#25퍼센트 pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "eAQ14i67Xzdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.25\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        cut = cut_node(param,percent)\n",
    "        net.state_dict()[name].data.copy_(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fKdJHWbX2DR",
    "outputId": "038c26f6-4ab3-4954-c104-504a9fdd2d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 69 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSiYXtBCYEJx",
    "outputId": "26710488-3eac-49fe-defb-67c03011007c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class plane is: 78.7 %\n",
      "Accuracy for class car   is: 88.3 %\n",
      "Accuracy for class bird  is: 52.2 %\n",
      "Accuracy for class cat   is: 38.9 %\n",
      "Accuracy for class deer  is: 66.8 %\n",
      "Accuracy for class dog   is: 68.8 %\n",
      "Accuracy for class frog  is: 65.4 %\n",
      "Accuracy for class horse is: 75.0 %\n",
      "Accuracy for class ship  is: 82.3 %\n",
      "Accuracy for class truck is: 81.4 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15퍼센트 pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "eAQ14i67Xzdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.15\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        cut = cut_node(param,percent)\n",
    "        net.state_dict()[name].data.copy_(cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fKdJHWbX2DR",
    "outputId": "038c26f6-4ab3-4954-c104-504a9fdd2d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 75 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSiYXtBCYEJx",
    "outputId": "26710488-3eac-49fe-defb-67c03011007c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class plane is: 78.2 %\n",
      "Accuracy for class car   is: 88.4 %\n",
      "Accuracy for class bird  is: 55.1 %\n",
      "Accuracy for class cat   is: 58.0 %\n",
      "Accuracy for class deer  is: 66.4 %\n",
      "Accuracy for class dog   is: 67.4 %\n",
      "Accuracy for class frog  is: 79.5 %\n",
      "Accuracy for class horse is: 85.3 %\n",
      "Accuracy for class ship  is: 85.8 %\n",
      "Accuracy for class truck is: 87.8 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10퍼센트 pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "eAQ14i67Xzdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.layer1.0.weight\n",
      "module.layer1.2.weight\n",
      "module.layer2.0.weight\n",
      "module.layer2.2.weight\n",
      "module.fc1.weight\n",
      "module.bn1.weight\n",
      "module.bn2.weight\n",
      "module.fc2.weight\n",
      "module.fc3.weight\n"
     ]
    }
   ],
   "source": [
    "percent = 0.10\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        cut = cut_node(param,percent)\n",
    "        net.state_dict()[name].data.copy_(cut)\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26848\n",
      "268500\n"
     ]
    }
   ],
   "source": [
    "global_parameters_count = 0\n",
    "global_zero_parameters_count = 0\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        loclal_parameters_count = param.nelement()\n",
    "        local_zero_parameters_count = torch.sum(param == 0)\n",
    "        global_parameters_count += loclal_parameters_count\n",
    "        global_zero_parameters_count += local_zero_parameters_count\n",
    "\n",
    "global_zero_parameters_count = int(global_zero_parameters_count)\n",
    "print(int(global_zero_parameters_count))\n",
    "print(global_parameters_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fKdJHWbX2DR",
    "outputId": "038c26f6-4ab3-4954-c104-504a9fdd2d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 76 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSiYXtBCYEJx",
    "outputId": "26710488-3eac-49fe-defb-67c03011007c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class plane is: 76.1 %\n",
      "Accuracy for class car   is: 88.8 %\n",
      "Accuracy for class bird  is: 62.6 %\n",
      "Accuracy for class cat   is: 55.8 %\n",
      "Accuracy for class deer  is: 65.9 %\n",
      "Accuracy for class dog   is: 68.6 %\n",
      "Accuracy for class frog  is: 83.7 %\n",
      "Accuracy for class horse is: 81.7 %\n",
      "Accuracy for class ship  is: 88.7 %\n",
      "Accuracy for class truck is: 90.0 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2퍼센트 pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "eAQ14i67Xzdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.layer1.0.weight\n",
      "module.layer1.2.weight\n",
      "module.layer2.0.weight\n",
      "module.layer2.2.weight\n",
      "module.fc1.weight\n",
      "module.bn1.weight\n",
      "module.bn2.weight\n",
      "module.fc2.weight\n",
      "module.fc3.weight\n"
     ]
    }
   ],
   "source": [
    "percent = 0.02\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        cut = cut_node(param,percent)\n",
    "        net.state_dict()[name].data.copy_(cut)\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "864\n",
      "184\n",
      "9216\n",
      "368\n",
      "18432\n",
      "737\n",
      "36864\n",
      "3840\n",
      "192000\n",
      "2\n",
      "120\n",
      "1\n",
      "84\n",
      "201\n",
      "10080\n",
      "16\n",
      "840\n",
      "5366\n",
      "268500\n"
     ]
    }
   ],
   "source": [
    "global_parameters_count = 0\n",
    "global_zero_parameters_count = 0\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        loclal_parameters_count = param.nelement()\n",
    "        local_zero_parameters_count = torch.sum(param == 0)\n",
    "        global_parameters_count += loclal_parameters_count\n",
    "        global_zero_parameters_count += local_zero_parameters_count\n",
    "        print(int(local_zero_parameters_count))\n",
    "        print(loclal_parameters_count)\n",
    "\n",
    "global_zero_parameters_count = int(global_zero_parameters_count)\n",
    "print(int(global_zero_parameters_count))\n",
    "print(global_parameters_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fKdJHWbX2DR",
    "outputId": "038c26f6-4ab3-4954-c104-504a9fdd2d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 78 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSiYXtBCYEJx",
    "outputId": "26710488-3eac-49fe-defb-67c03011007c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class plane is: 83.6 %\n",
      "Accuracy for class car   is: 88.1 %\n",
      "Accuracy for class bird  is: 65.3 %\n",
      "Accuracy for class cat   is: 61.1 %\n",
      "Accuracy for class deer  is: 74.3 %\n",
      "Accuracy for class dog   is: 65.3 %\n",
      "Accuracy for class frog  is: 83.5 %\n",
      "Accuracy for class horse is: 87.2 %\n",
      "Accuracy for class ship  is: 86.8 %\n",
      "Accuracy for class truck is: 91.5 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1퍼센트 pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "eAQ14i67Xzdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.layer1.0.weight\n",
      "module.layer1.2.weight\n",
      "module.layer2.0.weight\n",
      "module.layer2.2.weight\n",
      "module.fc1.weight\n",
      "module.bn1.weight\n",
      "module.bn2.weight\n",
      "module.fc2.weight\n",
      "module.fc3.weight\n"
     ]
    }
   ],
   "source": [
    "percent = 0.01\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        cut = cut_node(param,percent)\n",
    "        net.state_dict()[name].data.copy_(cut)\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "864\n",
      "92\n",
      "9216\n",
      "184\n",
      "18432\n",
      "368\n",
      "36864\n",
      "1920\n",
      "192000\n",
      "1\n",
      "120\n",
      "84\n",
      "84\n",
      "100\n",
      "10080\n",
      "8\n",
      "840\n",
      "2765\n",
      "268500\n"
     ]
    }
   ],
   "source": [
    "global_parameters_count = 0\n",
    "global_zero_parameters_count = 0\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        loclal_parameters_count = param.nelement()\n",
    "        local_zero_parameters_count = torch.sum(param == 0)\n",
    "        global_parameters_count += loclal_parameters_count\n",
    "        global_zero_parameters_count += local_zero_parameters_count\n",
    "        print(int(local_zero_parameters_count))\n",
    "        print(loclal_parameters_count)\n",
    "\n",
    "global_zero_parameters_count = int(global_zero_parameters_count)\n",
    "print(int(global_zero_parameters_count))\n",
    "print(global_parameters_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fKdJHWbX2DR",
    "outputId": "038c26f6-4ab3-4954-c104-504a9fdd2d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSiYXtBCYEJx",
    "outputId": "26710488-3eac-49fe-defb-67c03011007c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class plane is: 100.0 %\n",
      "Accuracy for class car   is: 0.0 %\n",
      "Accuracy for class bird  is: 0.0 %\n",
      "Accuracy for class cat   is: 0.0 %\n",
      "Accuracy for class deer  is: 0.0 %\n",
      "Accuracy for class dog   is: 0.0 %\n",
      "Accuracy for class frog  is: 0.0 %\n",
      "Accuracy for class horse is: 0.0 %\n",
      "Accuracy for class ship  is: 0.0 %\n",
      "Accuracy for class truck is: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0퍼센트 pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "eAQ14i67Xzdk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Net()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.layer1.0.weight\n",
      "module.layer1.2.weight\n",
      "module.layer2.0.weight\n",
      "module.layer2.2.weight\n",
      "module.fc1.weight\n",
      "module.bn1.weight\n",
      "module.bn2.weight\n",
      "module.fc2.weight\n",
      "module.fc3.weight\n"
     ]
    }
   ],
   "source": [
    "percent = 0.00\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        cut = cut_node(param,percent)\n",
    "        net.state_dict()[name].data.copy_(cut)\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864\n",
      "864\n",
      "9216\n",
      "9216\n",
      "18432\n",
      "18432\n",
      "36864\n",
      "36864\n",
      "192000\n",
      "192000\n",
      "120\n",
      "120\n",
      "84\n",
      "84\n",
      "10080\n",
      "10080\n",
      "840\n",
      "840\n",
      "268500\n",
      "268500\n"
     ]
    }
   ],
   "source": [
    "global_parameters_count = 0\n",
    "global_zero_parameters_count = 0\n",
    "for name, param in net.named_parameters():\n",
    "    if \"weight\" in name:       \n",
    "        loclal_parameters_count = param.nelement()\n",
    "        local_zero_parameters_count = torch.sum(param == 0)\n",
    "        global_parameters_count += loclal_parameters_count\n",
    "        global_zero_parameters_count += local_zero_parameters_count\n",
    "        print(int(local_zero_parameters_count))\n",
    "        print(loclal_parameters_count)\n",
    "\n",
    "global_zero_parameters_count = int(global_zero_parameters_count)\n",
    "print(int(global_zero_parameters_count))\n",
    "print(global_parameters_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fKdJHWbX2DR",
    "outputId": "038c26f6-4ab3-4954-c104-504a9fdd2d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "torch.Size([1024, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "torch.Size([1024, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "torch.Size([1024, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "torch.Size([1024, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "torch.Size([1024, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "torch.Size([1024, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "torch.Size([1024, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "torch.Size([1024, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "torch.Size([784, 10])\n",
      "tensor([[ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404],\n",
      "        [ 0.0896, -0.1501, -0.0386, -0.1110, -0.1103,  0.1418,  0.0755, -0.0124,\n",
      "          0.0092, -0.0404]], device='cuda:0')\n",
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "# 학습 중이 아니므로, 출력에 대한 변화도를 계산할 필요가 없습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 신경망에 이미지를 통과시켜 출력을 계산합니다\n",
    "        outputs = net(images)\n",
    "        print(outputs.size())\n",
    "        print(outputs[:2])\n",
    "        # 가장 높은 값(energy)를 갖는 분류(class)를 정답으로 선택하겠습니다\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zSiYXtBCYEJx",
    "outputId": "26710488-3eac-49fe-defb-67c03011007c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class plane is: 0.0 %\n",
      "Accuracy for class car   is: 0.0 %\n",
      "Accuracy for class bird  is: 0.0 %\n",
      "Accuracy for class cat   is: 0.0 %\n",
      "Accuracy for class deer  is: 0.0 %\n",
      "Accuracy for class dog   is: 100.0 %\n",
      "Accuracy for class frog  is: 0.0 %\n",
      "Accuracy for class horse is: 0.0 %\n",
      "Accuracy for class ship  is: 0.0 %\n",
      "Accuracy for class truck is: 0.0 %\n"
     ]
    }
   ],
   "source": [
    "# 각 분류(class)에 대한 예측값 계산을 위해 준비\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# 변화도는 여전히 필요하지 않습니다\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # 각 분류별로 올바른 예측 수를 모읍니다\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# 각 분류별 정확도(accuracy)를 출력합니다\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
    "                                                   accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_union2)",
   "language": "python",
   "name": "conda_union2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
